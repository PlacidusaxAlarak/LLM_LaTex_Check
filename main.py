# --- START OF FILE main.py (MODIFIED) ---

import os
import asyncio
import re
from datetime import datetime
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
from pathlib import Path

# NEW: ä» langchain_analysis_tool.py ç§»åŠ¨è¿‡æ¥çš„å¯¼å…¥
from langchain_core.tools import tool
from langchain_core.pydantic_v1 import BaseModel, Field

import llm_agent
from latex_parser import LatexProjectParser, find_bib_file_paths, parse_bib_files, extract_references_from_bbl
import file_writer
import archive_handler
import cache_handler

# --- é…ç½® ---
EXTRACT_DIR = './data'
OUTPUT_HTML_FILE = 'references_analysis_report.html'
BATCH_SIZE = 1
REFERENCE_PARSING_BATCH_SIZE = 1

# --- HTML æ¨¡æ¿ (ä¿æŒä¸å˜) ---
HTML_HEADER = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å‚è€ƒæ–‡çŒ®åˆ†ææŠ¥å‘Š - {title}</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; line-height: 1.6; color: #333; background-color: #f8f9fa; margin: 0; padding: 20px; }}
        .container {{ max-width: 900px; margin: 0 auto; background-color: #fff; padding: 2rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); }}
        h1, h2, h3 {{ color: #0056b3; }}
        h1 {{ border-bottom: 2px solid #dee2e6; padding-bottom: 0.5rem; margin-bottom: 1rem; text-align: center; }}
        h2 em {{ font-weight: normal; color: #555; font-size: 0.8em; }}
        .reference-item {{ margin-bottom: 2rem; padding: 1.5rem; border: 1px solid #e9ecef; border-radius: 6px; background-color: #ffffff; transition: box-shadow 0.3s ease; }}
        .reference-item:hover {{ box-shadow: 0 8px 16px rgba(0,0,0,0.1); }}
        blockquote {{ margin: 0; padding: 1rem; background-color: #f8f9fa; border-left: 5px solid #007bff; }}
        ul {{ padding-left: 20px; }}
        li {{ margin-bottom: 0.5rem; }}
        code {{ background-color: #e9ecef; color: #d63384; padding: 2px 4px; border-radius: 3px; }}
        .citation-context li strong:first-child {{ color: #28a745; }}
        .citation-context strong {{ color: #d9534f; }}
        .footer {{ text-align: center; margin-top: 2rem; font-size: 0.9em; color: #6c757d; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>å‚è€ƒæ–‡çŒ®ä¸Šä¸‹æ–‡åˆ†ææŠ¥å‘Š</h1>
        <h2>è®ºæ–‡: <em>{title}</em></h2>
"""
HTML_FOOTER = """
    </div>
    <div class="footer">
        <p>æŠ¥å‘Šç”Ÿæˆäº: {timestamp}</p>
    </div>
</body>
</html>
"""


# NEW: ä» langchain_analysis_tool.py ç§»åŠ¨è¿‡æ¥çš„ Pydantic æ¨¡å‹
class LatexAnalysisInput(BaseModel):
    """ç”¨æ–¼ LaTeX åˆ†æå·¥å…·çš„è¼¸å…¥æ¨¡å‹ã€‚"""
    archive_path: str = Field(description="å¿…é ˆæ˜¯æŒ‡å‘ LaTeX é …ç›®æ­¸æª”æ–‡ä»¶ï¼ˆå¦‚ .zip, .tar.gzï¼‰çš„æœ‰æ•ˆè·¯å¾‘ã€‚")


def _clean_latex_for_llm(latex_content: str) -> str:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    print("   â””â”€â”€ æ­£åœ¨å¯¹LaTeXæºç è¿›è¡Œé¢„æ¸…ç†ä»¥ä¼˜åŒ–åˆ†æ...")
    cleaned_content = re.sub(r'(?<!\\)%.*\n', '\n', latex_content)
    cleaned_content = re.sub(r'\\begin{comment}(.*?)\\end{comment}', '', cleaned_content, flags=re.DOTALL)
    return cleaned_content


def render_html_from_data(all_references_data: list[dict]) -> str:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    html_parts = []
    sorted_data = sorted(all_references_data, key=lambda x: x.get('id', 0))
    for item in sorted_data:
        key = item.get("key", "N/A")
        title = item.get("inferred_title", item.get("title", "N/A"))
        author = item.get("inferred_author", "ä½œè€…ä¿¡æ¯æœªæå–")
        source = item.get("inferred_source", item.get("content", ""))

        item_html = f'<div class="reference-item"><h3>å‚è€ƒæ–‡çŒ®: <code>{key}</code></h3><blockquote><p><strong>ä½œè€…:</strong> {author}</p><p><strong>æ ‡é¢˜:</strong> {title}</p><p><strong>æ¥æº:</strong> {source}</p></blockquote><h4>å¼•ç”¨ä½ç½®:</h4>'

        if item.get("analysis_failed"):
            item_html += '<p><em style="color: red;">æ­¤å‚è€ƒæ–‡çŒ®çš„ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥ã€‚</em></p>'
        elif not item.get("citations"):
            item_html += '<p><em>æ­£æ–‡ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆå¼•ç”¨ã€‚</em></p>'
        else:
            item_html += '<ul>'
            for i, citation in enumerate(item.get("citations", []), 1):
                section = citation.get("section", "Unknown Section")
                pre_context = citation.get("pre_context", "")
                citation_sentence = citation.get("citation_sentence", "")

                escaped_key = re.escape(key)
                pattern = r'(\\(?:cite|citep|citet|autocite)\*?\{[^{}]*?' + escaped_key + r'[^}]*?\})'
                citation_sentence_html = re.sub(pattern, r'<strong>\1</strong>', citation_sentence)

                post_context = citation.get("post_context", "")
                item_html += f'<li><strong>ä½ç½® {i}:</strong><ul class="citation-context"><li><strong>ç« èŠ‚:</strong> {section}</li><li><strong>å‰æ–‡:</strong> {pre_context}</li><li><strong>å¼•æ–‡å¥:</strong> {citation_sentence_html}</li><li><strong>åæ–‡:</strong> {post_context}</li></ul></li>'
            item_html += '</ul>'

        item_html += '</div>'
        html_parts.append(item_html)

    return "".join(html_parts)


async def get_references_from_llm(agent: llm_agent.LLMAgent, text_block: str) -> List[Dict]:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    bib_items_raw = re.split(r'\\bibitem', text_block)[1:]
    bib_items = ["\bibitem" + item for item in bib_items_raw if item.strip()]
    if not bib_items:
        return []

    print(f"   â””â”€â”€ å·²å°†å†…å®¹æ‹†åˆ†ä¸º {len(bib_items)} ä¸ªç‹¬ç«‹çš„å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼Œäº¤ç”±LLMå¤„ç†ã€‚")
    tasks = [agent.run_reference_parser("".join(bib_items[i:i + REFERENCE_PARSING_BATCH_SIZE])) for i in
             range(0, len(bib_items), REFERENCE_PARSING_BATCH_SIZE)]
    parsed_batches = await asyncio.gather(*tasks)

    return [ref for batch in parsed_batches for ref in batch]


# MODIFIED: å°†åŸæ¥çš„ main å‡½æ•°é‡æ„ä¸º LangChain Tool
@tool(args_schema=LatexAnalysisInput)
async def analyze_latex_references(archive_path: str) -> str:
    """
    åˆ†ææŒ‡å®šçš„ LaTeX é …ç›®æ­¸æª”æ–‡ä»¶ï¼Œä»¥æå–æ‰€æœ‰åƒè€ƒæ–‡ç»ä¸¦æ‰¾å‡ºå®ƒå€‘åœ¨æ­£æ–‡ä¸­çš„å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚

    æ­¤å·¥å…·æœƒåŸ·è¡Œä»¥ä¸‹æ“ä½œï¼š
    1. è§£å£“æ­¸æª”æ–‡ä»¶ã€‚
    2. æ™ºèƒ½åˆä½µæ‰€æœ‰ .tex æºæ–‡ä»¶ã€‚
    3. è§£æåƒè€ƒæ–‡ç»åˆ—è¡¨ï¼ˆä¾†è‡ª .bib æˆ– .bbl æ–‡ä»¶ï¼‰ã€‚
    4. ä½¿ç”¨ LLM éæ­·æºä»£ç¢¼ï¼Œç‚ºæ¯ç¯‡åƒè€ƒæ–‡ç»å®šä½æ‰€æœ‰å¼•ç”¨é»åŠå…¶ä¸Šä¸‹æ–‡ã€‚
    5. ç”Ÿæˆä¸€ä»½è©³ç´°çš„ HTML å ±å‘Šã€‚

    æˆåŠŸæ™‚è¿”å›å ±å‘Šè·¯å¾‘å’Œæ‘˜è¦ï¼›å¤±æ•—æ™‚è¿”å›éŒ¯èª¤ä¿¡æ¯ã€‚
    """
    print(f"--- ğŸš€ LangChain Tool: 'analyze_latex_references' å·²å•Ÿå‹• ---")
    print(f"--- ğŸ¯ è¼¸å…¥æ–‡ä»¶: {archive_path} ---")

    load_dotenv(".env")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        error_msg = "é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEYã€‚"
        print(f"--- âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ ---")
        print(error_msg)
        return error_msg

    cache_handler.ensure_cache_dir_exists()
    agent = llm_agent.LLMAgent(api_key=api_key)

    try:
        # MODIFIED: ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ archive_pathï¼Œç§»é™¤äº†è‡ªåŠ¨æŸ¥æ‰¾æ–‡ä»¶çš„é€»è¾‘
        source_archive_path = Path(archive_path)
        if not source_archive_path.exists():
            raise FileNotFoundError(f"æŒ‡å®šçš„å½’æ¡£æ–‡ä»¶æœªæ‰¾åˆ°: {archive_path}")

        print(f"âœ… ä½¿ç”¨æºå½’æ¡£æ–‡ä»¶: {source_archive_path.name}")

        # Step 1: è§£å‹
        print(f"\næ­¥éª¤ 1: æ­£åœ¨è§£å‹...", flush=True)
        archive_handler.extract_archive(str(source_archive_path), EXTRACT_DIR)

        # Step 2: è§£æé¡¹ç›®ç»“æ„
        print(f"\næ­¥éª¤ 2: æ­£åœ¨è§£æé¡¹ç›®ç»“æ„...", flush=True)
        parser = LatexProjectParser(EXTRACT_DIR)
        parser.parse()

        full_latex_content = parser.latex_verbatim_content
        if not full_latex_content or not parser.main_file:
            raise RuntimeError("è§£æé¡¹ç›®å¤±è´¥ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹æˆ–ä¸»æ–‡ä»¶ã€‚")

        cleaned_latex_content = _clean_latex_for_llm(full_latex_content)

        # Step 3: è§£æå‚è€ƒæ–‡çŒ®
        print("\næ­¥éª¤ 3: æ­£åœ¨è§£æå‚è€ƒæ–‡çŒ®...", flush=True)
        all_references = []
        if parser.bib_file_names:
            print("   â””â”€â”€ ç­–ç•¥: æ‰¾åˆ° .bib æ–‡ä»¶å¼•ç”¨ï¼Œä½¿ç”¨ bibtexparser ç²¾å‡†è§£æã€‚")
            bib_paths = find_bib_file_paths(parser.bib_file_names, Path(EXTRACT_DIR))
            if bib_paths:
                all_references, _ = parse_bib_files(bib_paths)

        if not all_references:
            print("   â””â”€â”€ ç­–ç•¥: å›é€€åˆ°LLMè§£æ .bbl æˆ– .tex å†…å®¹ã€‚")
            references_text_block = parser.the_bibliography_content or extract_references_from_bbl(parser.main_file)
            if not references_text_block:
                raise ValueError("åœ¨é¡¹ç›®ä¸­æ‰¾ä¸åˆ°ä»»ä½•å‚è€ƒæ–‡çŒ®ä¿¡æ¯ã€‚")
            all_references = await get_references_from_llm(agent, references_text_block)

        if not all_references:
            raise ValueError("æœªèƒ½è§£æå‡ºä»»ä½•å‚è€ƒæ–‡çŒ®ã€‚")

        for i, ref in enumerate(all_references, 1):
            ref['id'] = i
        total_refs = len(all_references)
        print(f"âœ… æˆåŠŸè·å¾— {total_refs} æ¡ç»“æ„åŒ–å‚è€ƒæ–‡çŒ®ã€‚")

        # Step 4 & 5: å¹¶å‘åˆ†æå¼•ç”¨ä¸Šä¸‹æ–‡
        print(f"\næ­¥éª¤ 4 & 5: æ­£åœ¨å¹¶å‘åˆ†æå¼•ç”¨ä¸Šä¸‹æ–‡...", flush=True)
        tasks = [agent.run_extraction_batch(cleaned_latex_content, [ref]) for ref in all_references]
        structured_data_chunks = await asyncio.gather(*tasks)

        # Step 6: åˆå¹¶ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
        print("\næ­¥éª¤ 6: æ­£åœ¨åˆå¹¶ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š...", flush=True)
        final_data_map = {ref['key']: ref for ref in all_references}
        successful_extractions = 0

        for i, chunk in enumerate(structured_data_chunks):
            ref_key = all_references[i]['key']
            if chunk and (results_list := chunk.get("analysis_results")) and isinstance(results_list, list) and len(
                    results_list) > 0:
                result_data = results_list[0]
                if (key := result_data.get('key')) in final_data_map:
                    if result_data.get("citations"):
                        successful_extractions += 1
                    final_data_map[key].update(result_data)
            else:
                final_data_map[ref_key]['analysis_failed'] = True

        print(f"--- åˆ†ææ‘˜è¦: åœ¨ {total_refs} ä¸ªå‚è€ƒæ–‡çŒ®ä¸­ï¼Œæœ‰ {successful_extractions} ä¸ªæˆåŠŸæ‰¾åˆ°äº†è‡³å°‘ä¸€å¤„å¼•ç”¨ã€‚---")

        full_html = render_html_from_data(list(final_data_map.values()))
        title = parser.paper_title if parser.paper_title != "æœªæ‰¾åˆ°æ ‡é¢˜" else "æœªå‘½åæ–‡æ¡£"
        header = HTML_HEADER.format(title=title)
        footer = HTML_FOOTER.format(timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        file_writer.save_html_report(header + full_html + footer, OUTPUT_HTML_FILE)

        print(f"\nğŸ‰ å·¥ä½œæµç¨‹å®Œæˆï¼è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ '{OUTPUT_HTML_FILE}' æŸ¥çœ‹æŠ¥å‘Šã€‚")

        # MODIFIED: è¿”å›å¯¹ Agent å‹å¥½çš„å­—ç¬¦ä¸²æ‘˜è¦
        summary = f"âœ… æˆåŠŸå®Œæˆå¯¹ '{archive_path}' çš„åˆ†æã€‚æŠ¥å‘Šå·²ä¿å­˜è‡³ '{OUTPUT_HTML_FILE}'ã€‚å…±æ‰¾åˆ°å¹¶å¤„ç†äº† {len(all_references)} æ¡å‚è€ƒæ–‡ç»ã€‚"
        print(f"--- âœ¨ å·¥å…·æ‰§è¡ŒæˆåŠŸ ---")
        print(summary)
        return summary

    except Exception as e:
        import traceback
        traceback.print_exc()
        error_summary = f"âŒ åœ¨åˆ†æ '{archive_path}' éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"
        print(f"--- ğŸ’¥ å·¥å…·æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸ ---")
        print(error_summary)
        return error_summary


# MODIFIED: æ›´æ–° main block ä»¥ä¾¿ç›´æ¥æµ‹è¯•æ–°çš„ LangChain tool
async def example_usage():
    """å±•ç¤ºå¦‚ä½•ç›´æ¥èª¿ç”¨é€™å€‹ LangChain å·¥å…·ã€‚"""
    from pathlib import Path
    supported_extensions = ('.zip', '.tar', '.gz', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2')
    project_dir = Path('.')
    found_archives = [p for p in project_dir.iterdir() if p.is_file() and str(p.name).endswith(supported_extensions)]

    if not found_archives:
        print("\n--- ç¤ºä¾‹é‹è¡Œå¤±æ•— ---")
        print("è«‹åœ¨é …ç›®æ ¹ç›®éŒ„ä¸‹æ”¾ç½®ä¸€å€‹ LaTeX é …ç›®çš„ .zip æˆ– .tar.gz æ­¸æª”æ–‡ä»¶ä»¥é‹è¡Œæ­¤ç¤ºä¾‹ã€‚")
        return

    example_archive_path = str(found_archives[0])
    print("\n" + "=" * 50)
    print("      å±•ç¤ºå¦‚ä½•èª¿ç”¨å·²åœ¨ main.py ä¸­å®šä¹‰çš„ LangChain Tool")
    print("=" * 50)

    # æ¨¡æ‹Ÿ LangChain Agent èª¿ç”¨å·¥å…·
    result = await analyze_latex_references.ainvoke({"archive_path": example_archive_path})

    print("\n--- å·¥å…·è¿”å›çš„æœ€çµ‚çµæœ ---")
    print(result)
    print("=" * 50)


if __name__ == "__main__":
    asyncio.run(example_usage())