import os
import asyncio
import re
from datetime import datetime
from dotenv import load_dotenv
from typing import List, Dict, Any
from pathlib import Path
import llm_agent
import latex_parser
import file_writer
import archive_handler
import cache_handler

# --- é…ç½® ---
EXTRACT_DIR = './data'
OUTPUT_HTML_FILE = 'references_analysis_report.html'
BATCH_SIZE = 1
REFERENCE_PARSING_BATCH_SIZE = 1

# --- HTML æ¨¡æ¿ (ä¿æŒä¸å˜) ---
# MODIFICATION: Escaped CSS braces by doubling them up (e.g., { -> {{)
HTML_HEADER = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å‚è€ƒæ–‡çŒ®åˆ†ææŠ¥å‘Š - {title}</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; line-height: 1.6; color: #333; background-color: #f8f9fa; margin: 0; padding: 20px; }}
        .container {{ max-width: 900px; margin: 0 auto; background-color: #fff; padding: 2rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); }}
        h1, h2, h3 {{ color: #0056b3; }}
        h1 {{ border-bottom: 2px solid #dee2e6; padding-bottom: 0.5rem; margin-bottom: 1rem; text-align: center; }}
        h2 em {{ font-weight: normal; color: #555; font-size: 0.8em; }}
        .reference-item {{ margin-bottom: 2rem; padding: 1.5rem; border: 1px solid #e9ecef; border-radius: 6px; background-color: #ffffff; transition: box-shadow 0.3s ease; }}
        .reference-item:hover {{ box-shadow: 0 8px 16px rgba(0,0,0,0.1); }}
        blockquote {{ margin: 0; padding: 1rem; background-color: #f8f9fa; border-left: 5px solid #007bff; }}
        ul {{ padding-left: 20px; }}
        li {{ margin-bottom: 0.5rem; }}
        code {{ background-color: #e9ecef; color: #d63384; padding: 2px 4px; border-radius: 3px; }}
        .citation-context li strong:first-child {{ color: #28a745; }}
        .citation-context strong {{ color: #d9534f; }}
        .footer {{ text-align: center; margin-top: 2rem; font-size: 0.9em; color: #6c757d; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>å‚è€ƒæ–‡çŒ®ä¸Šä¸‹æ–‡åˆ†ææŠ¥å‘Š</h1>
        <h2>è®ºæ–‡: <em>{title}</em></h2>
"""
HTML_FOOTER = """
    </div>
    <div class="footer">
        <p>æŠ¥å‘Šç”Ÿæˆäº: {timestamp}</p>
    </div>
</body>
</html>
"""

def extract_paper_title(latex_content: str) -> str:
    # No changes here
    match = re.search(r'\\title\{([^}]+)\}', latex_content, re.DOTALL)
    if match:
        title = match.group(1).replace('\\', ' ').replace('\n', ' ').strip()
        return re.sub(r'\s+', ' ', title)
    return "æœªæ‰¾åˆ°æ ‡é¢˜"

def render_html_from_data(all_references_data: list[dict]) -> str:
    # No changes here
    html_parts = []
    sorted_data = sorted(all_references_data, key=lambda x: x.get('id', 0))
    for item in sorted_data:
        key = item.get("key", "N/A")
        title = item.get("inferred_title", item.get("title", "N/A"))
        author = item.get("inferred_author", "ä½œè€…ä¿¡æ¯æœªæå–")
        source = item.get("inferred_source", item.get("content", ""))
        item_html = f'<div class="reference-item">\n'
        item_html += f'    <h3>å‚è€ƒæ–‡çŒ®: <code>{key}</code></h3>\n'
        item_html += f'    <blockquote>\n'
        item_html += f'        <p><strong>ä½œè€…:</strong> {author}</p>\n'
        item_html += f'        <p><strong>æ ‡é¢˜:</strong> {title}</p>\n'
        item_html += f'        <p><strong>æ¥æº:</strong> {source}</p>\n'
        item_html += f'    </blockquote>\n'
        item_html += '    <h4>å¼•ç”¨ä½ç½®:</h4>\n'
        if item.get("analysis_failed"):
            item_html += '<p><em style="color: red;">æ­¤å‚è€ƒæ–‡çŒ®çš„ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥ã€‚</em></p>\n'
        elif not item.get("citations"):
            item_html += '<p><em>æ­£æ–‡ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆå¼•ç”¨ã€‚</em></p>\n'
        else:
            item_html += '    <ul>\n'
            for i, citation in enumerate(item.get("citations", []), 1):
                section = citation.get("section", "Unknown Section")
                pre_context = citation.get("pre_context", "")
                citation_sentence = citation.get("citation_sentence", "")
                citation_sentence_html = re.sub(
                    r'(\\(?:cite|citep|citet|Citep|Citet|citealt)\*?\{[^}]*?' + re.escape(key) + r'[^}]*?\})',
                    r'<strong>\1</strong>', citation_sentence)
                post_context = citation.get("post_context", "")
                item_html += f'        <li>\n'
                item_html += f'            <strong>ä½ç½® {i}:</strong>\n'
                item_html += f'            <ul class="citation-context">\n'
                item_html += f'                <li><strong>ç« èŠ‚:</strong> {section}</li>\n'
                item_html += f'                <li><strong>å‰æ–‡:</strong> {pre_context}</li>\n'
                item_html += f'                <li><strong>å¼•æ–‡å¥:</strong> {citation_sentence_html}</li>\n'
                item_html += f'                <li><strong>åæ–‡:</strong> {post_context}</li>\n'
                item_html += f'            </ul>\n'
                item_html += f'        </li>\n'
            item_html += '    </ul>\n'
        item_html += '</div>\n'
        html_parts.append(item_html)
    return "".join(html_parts)

async def get_references_from_llm(agent: llm_agent.LLMAgent, text_block: str) -> List[Dict]:
    # No changes here
    bib_items_raw = re.split(r'\bibitem', text_block)[1:]
    bib_items = ["\bibitem" + item for item in bib_items_raw if item.strip()]
    if not bib_items:
        print("âŒ è‡´å‘½é”™è¯¯: æœªèƒ½ä».bblæ–‡ä»¶å†…å®¹ä¸­æ‹†åˆ†å‡ºä»»ä½• \bibitem æ¡ç›®ã€‚")
        return []

    print(f"   â””â”€â”€ å·²å°†å†…å®¹æ‹†åˆ†ä¸º {len(bib_items)} ä¸ªç‹¬ç«‹çš„å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼Œäº¤ç”±LLMå¤„ç†ã€‚")
    parsing_tasks = []
    for i in range(0, len(bib_items), REFERENCE_PARSING_BATCH_SIZE):
        batch_of_bib_items = bib_items[i:i + REFERENCE_PARSING_BATCH_SIZE]
        batch_content = "".join(batch_of_bib_items)
        task = agent.run_reference_parser(batch_content)
        parsing_tasks.append(task)

    parsed_batches = await asyncio.gather(*parsing_tasks)
    all_references = []
    for batch_result in parsed_batches:
        all_references.extend(batch_result)
    return all_references

async def main() -> List[Dict[str, Any]]:
    # No changes here, the logic is sound
    load_dotenv(".env")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEYã€‚")
        return []

    cache_handler.ensure_cache_dir_exists()
    agent = llm_agent.LLMAgent(api_key=api_key)

    try:
        supported_extensions = ('.zip', '.tar', '.gz', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2')
        project_dir = Path('.')
        found_archives = [p for p in project_dir.iterdir() if p.is_file() and str(p.name).endswith(supported_extensions)]
        if not found_archives:
            print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ”¯æŒçš„å½’æ¡£æ–‡ä»¶ã€‚")
            return []
        source_archive_path = found_archives[0]
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°æºå½’æ¡£æ–‡ä»¶: {source_archive_path.name}")

        print(f"\næ­¥éª¤ 1 & 2: æ­£åœ¨è§£å‹ä¸æ•´åˆæºæ–‡ä»¶...", flush=True)
        archive_handler.extract_archive(str(source_archive_path), EXTRACT_DIR)
        full_latex_content, main_file = latex_parser.get_full_latex_source_and_main_file(EXTRACT_DIR)
        if not full_latex_content or not main_file:
            return []
        paper_title = extract_paper_title(full_latex_content)
        print(f"\nâœ… æˆåŠŸæå–è®ºæ–‡æ ‡é¢˜: {paper_title}")

        print("\næ­¥éª¤ 3: æ­£åœ¨è§£æå‚è€ƒæ–‡çŒ®...", flush=True)
        all_references = []
        try:
            bib_file_paths = latex_parser.find_bib_file_paths(full_latex_content, Path(EXTRACT_DIR))
            if bib_file_paths:
                print("   â””â”€â”€ ç­–ç•¥: æ‰¾åˆ° .bib æ–‡ä»¶ï¼Œä½¿ç”¨ bibtexparser ç²¾å‡†è§£æã€‚")
                parsed_refs, bib_content = latex_parser.parse_bib_files(bib_file_paths)
                if parsed_refs:
                    all_references = parsed_refs
                    cache_key = cache_handler.get_cache_key(bib_content)
                    cache_handler.set_to_cache(cache_key, all_references)
                else:
                    print("   â””â”€â”€ âš ï¸ .bib æ–‡ä»¶è§£æå¤±è´¥æˆ–å†…å®¹ä¸ºç©ºã€‚")

            if not all_references:
                print("   â””â”€â”€ ç­–ç•¥: æœªä½¿ç”¨.bibæ–‡ä»¶æˆ–è§£æå¤±è´¥ï¼Œå›é€€åˆ°LLMè§£æ.bbl/.texå†…å®¹ã€‚")
                references_text_block = latex_parser.extract_raw_references_text(full_latex_content, main_file)
                all_references = await get_references_from_llm(agent, references_text_block)

        except ImportError:
            print("   â””â”€â”€ âš ï¸ è­¦å‘Š: `bibtexparser` æœªå®‰è£…ã€‚å›é€€åˆ°LLMè§£æã€‚")
            print("   â””â”€â”€ è¯·è¿è¡Œ `pip install bibtexparser` ä»¥è·å¾—æ›´å‡†ç¡®çš„è§£æç»“æœã€‚")
            references_text_block = latex_parser.extract_raw_references_text(full_latex_content, main_file)
            all_references = await get_references_from_llm(agent, references_text_block)

        if not all_references:
            print("âŒ è‡´å‘½é”™è¯¯: æœªèƒ½è§£æå‡ºä»»ä½•å‚è€ƒæ–‡çŒ®ã€‚å·¥ä½œæµç¨‹ä¸­æ­¢ã€‚")
            return []

        for i, ref in enumerate(all_references, 1):
            ref['id'] = i
        total_refs = len(all_references)
        print(f"âœ… æˆåŠŸè·å¾— {total_refs} æ¡ç»“æ„åŒ–å‚è€ƒæ–‡çŒ®ã€‚")

        print(f"\næ­¥éª¤ 4 & 5: æ­£åœ¨å¹¶å‘åˆ†æ {total_refs} æ¡å‚è€ƒæ–‡çŒ®çš„å¼•ç”¨ä¸Šä¸‹æ–‡...", flush=True)
        extraction_tasks = [agent.run_extraction_batch(full_latex_content, [ref]) for ref in all_references]
        structured_data_chunks = await asyncio.gather(*extraction_tasks)

        print("\næ­¥éª¤ 6: æ­£åœ¨åˆå¹¶ç»“æœå¹¶ç”ŸæˆHTMLæŠ¥å‘Š...", flush=True)
        final_data_map = {ref['key']: ref for ref in all_references}
        for i, chunk in enumerate(structured_data_chunks):
            ref_key = all_references[i]['key']
            if chunk is None:
                final_data_map[ref_key]['analysis_failed'] = True
                continue
            for result in chunk.get("analysis_results", []):
                key = result.get('key')
                if key in final_data_map:
                    final_data_map[key].update(result)

        full_html_content = render_html_from_data(list(final_data_map.values()))
        header = HTML_HEADER.format(title=paper_title)
        footer = HTML_FOOTER.format(timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        file_writer.save_html_report(header + full_html_content + footer, OUTPUT_HTML_FILE)

        output_reference_data = []
        for key, data in final_data_map.items():
            sections = sorted(list(set(c.get('section', 'N/A') for c in data.get('citations', []))))
            output_reference_data.append({"key": key, "title": data.get("inferred_title", "N/A"), "sections": sections})

        print(f"\nğŸ‰ å·¥ä½œæµç¨‹å®Œæˆï¼è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ '{OUTPUT_HTML_FILE}' æŸ¥çœ‹æŠ¥å‘Šã€‚")
        return output_reference_data

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿæ„å¤–çš„è‡´å‘½é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []

if __name__ == "__main__":
    asyncio.run(main())